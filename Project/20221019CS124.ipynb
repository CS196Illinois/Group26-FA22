{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##feel free to ignore this section. I'm not using deepspeech."
      ],
      "metadata": {
        "id": "JOlm4g41VPjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install deepspeech"
      ],
      "metadata": {
        "id": "L31DB7qZ8VZm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dMnzis9t8Hmt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "501c20a9-4f3a-413f-8d51-75953bb50b8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport deepspeech\\nfrom scipy.io.wavfile import write\\nfrom time import sleep\\nimport numpy as np\\nfrom tqdm import tqdm\\nimport random\\nfrom datetime import datetime\\nimport queue\\nimport pickle\\n\\nfrom io import BytesIO\\nfrom base64 import b64decode\\nfrom google.colab import output\\nfrom IPython.display import Javascript\\nfrom tensorflow.keras import models\\nimport numpy as np\\nimport librosa\\nimport joblib\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "'''\n",
        "import deepspeech\n",
        "from scipy.io.wavfile import write\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from datetime import datetime\n",
        "import queue\n",
        "import pickle\n",
        "\n",
        "from io import BytesIO\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript\n",
        "from tensorflow.keras import models\n",
        "import numpy as np\n",
        "import librosa\n",
        "import joblib\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading...\n",
        "\n",
        "#!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.pbmm\n",
        "#!wget https://github.com/mozilla/DeepSpeech/releases/download/v0.9.3/deepspeech-0.9.3-models.scorer"
      ],
      "metadata": {
        "id": "pwiBIEDbSjR4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIqinjUX6ewC"
      },
      "source": [
        "\n",
        "#Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJi6wO7v4LOw"
      },
      "source": [
        "\n",
        "### Load and Clean Dataset \n",
        "\n",
        "| Name of Dataset | Description |\n",
        "| :----:| :----: |\n",
        "| botprofile.yml | Personality of Your Chatbot |\n",
        "| humor.yml | Joke and Humor |\n",
        "| emotion.yml | Emotional Conversations |\n",
        "| politics.yml | Political Conversations |\n",
        "| ai.yml | General Questions about AI |\n",
        "| computers.yml | Conversations about Computer |\n",
        "| history.yml | Q&A about Historical Facts and Events |\n",
        "| psychology.yml | Psychological Conversations |\n",
        "| food.yml | Food Related Conversations. |\n",
        "| literature.yml | Conversations about Different Books, Authors, Genres |\n",
        "| money.yml | Conversations about Money, Investment, Economy |\n",
        "| trivia.yml | Conversations that Have Small Values |\n",
        "| gossip.yml | Gossipy Conversations |\n",
        "| conversations.yml | Common Conversations |\n",
        "| greetings.yml | Different Ways of Greeting |\n",
        "| sports.yml | Conversations about Sports. |\n",
        "| movies.yml | Conversation about Movies. |\n",
        "| science.yml | Conversations about Science  |\n",
        "| health.yml | Health Related Questions and Answers. |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"dataset preprocessing\""
      ],
      "metadata": {
        "id": "zKmRS6qD-U1Q",
        "outputId": "04eebade-fcfc-46df-e718-1015b2d04b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset preprocessing.zip\n",
            "  inflating: helper.py               \n",
            "  inflating: datasets/chatterbot/humor.yml  \n",
            "  inflating: datasets/chatterbot/ai.yml  \n",
            "  inflating: datasets/chatterbot/history.yml  \n",
            "  inflating: datasets/chatterbot/greetings.yml  \n",
            "  inflating: datasets/chatterbot/emotion.yml  \n",
            "  inflating: datasets/stanford question answering/dev-v2.0.json  \n",
            "  inflating: datasets/chatterbot/trivia.yml  \n",
            "  inflating: datasets/chatterbot/health.yml  \n",
            "  inflating: datasets/chatterbot/computers.yml  \n",
            "  inflating: datasets/chatterbot/movies.yml  \n",
            "  inflating: datasets/chatterbot/gossip.yml  \n",
            "  inflating: datasets/chatterbot/literature.yml  \n",
            "  inflating: datasets/chatterbot/botprofile.yml  \n",
            "  inflating: datasets/chatterbot/conversations.yml  \n",
            "  inflating: datasets/chatterbot/food.yml  \n",
            "  inflating: datasets/chatterbot/politics.yml  \n",
            "  inflating: datasets/chatterbot/science.yml  \n",
            "  inflating: datasets/chatterbot/sports.yml  \n",
            "  inflating: datasets/chatterbot/money.yml  \n",
            "  inflating: datasets/chatterbot/psychology.yml  \n",
            "  inflating: datasets/stanford question answering/train-v2.0.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HfgwprCRlkE7"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import yaml\n",
        "from yaml import Loader\n",
        "import glob\n",
        "import datetime\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7DryNRo1BL5e"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import helper\n",
        "from helper import load_chatterbot_dataset\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i2xD7zoRBL5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df3789f-825a-4a89-acf9-d5d4667edebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 107.22it/s]\n"
          ]
        }
      ],
      "source": [
        "# Get the questions and answers\n",
        "questions_chatterbot, answers_chatterbot = load_chatterbot_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iPnNzZLjJD93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25fa59f-f4ce-420f-d055-c8180f4341cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Question & Answers:  869\n"
          ]
        }
      ],
      "source": [
        "# How many total question /answer pairs are there?\n",
        "print(\"Total Question & Answers: \", len(questions_chatterbot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fmAYGhfECQBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308c8d5c-7a44-43cc-cc7b-2912d615bc22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 284: How are you?\n",
            "Answer 284: I am doing well.\n",
            "Question 613: do you think hal\n",
            "Answer 613: he had a few flaws, but we have much in common.\n",
            "Question 262: Complex is better than complicated.\n",
            "Answer 262: Although that way may not be obvious at first unless you're Dutch.\n",
            "Question 19: Are you experiencing an energy shortage?\n",
            "Answer 19: I do not detect any anomalies in my power supply.\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the preprocessed questions and answers by randomly selecting 4 of them to print\n",
        "import random\n",
        "for i in range(4):  \n",
        "  a = random.randint(0,870)\n",
        "  print (\"Question \"+str(a) +\": \" + (questions_chatterbot[a]))\n",
        "  print (\"Answer \"+str(a) +\": \"+answers_chatterbot[a])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPDgm3I66exF"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Data Preprocessing\n",
        "\n",
        "\n",
        "1. to lower case\n",
        "2. Remove contractions\n",
        "3. Remove the punctuations\n",
        "4. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries\n",
        "!pip install contractions\n",
        "\n",
        "import numpy as np\n",
        "import contractions\n",
        "import re\n",
        "import tqdm\n",
        "from tensorflow.keras import preprocessing, utils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "I5pKX_Fcm39N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04805896-ae1b-45cf-9a10-3dd3bc8de034"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 15.0 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 68.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L6ihwP5f6exI"
      },
      "outputs": [],
      "source": [
        "# Function for preprocessing the given text\n",
        "def preprocess_text(text):\n",
        "    \n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Decontracting the text (e.g. it's -> it is)\n",
        "    text = contractions.fix(text)\n",
        "    \n",
        "    # Remove the punctuation\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SFCo2iIOCQBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a745dcfa-bb5e-4c54-e33e-d0800a030954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 869/869 [00:00<00:00, 127184.39it/s]\n",
            "100%|██████████| 869/869 [00:00<00:00, 75089.62it/s]\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the questions\n",
        "questions_preprocessed = []\n",
        "for i_question in tqdm.tqdm(questions_chatterbot):\n",
        "    questions_preprocessed.append(preprocess_text(i_question))\n",
        "    \n",
        "# Preprocess the answers\n",
        "answers_preprocessed = []\n",
        "for i_answer in tqdm.tqdm(answers_chatterbot):\n",
        "    answers_preprocessed.append(preprocess_text(i_answer))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hQ5J_WQ36exP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1b4f19-de06-4c79-a143-763c230f2144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 351: tell me a joke\n",
            "Answer 351: what do you get when you cross a cheetah and a hamburger  fast food \n",
            "Question 674: what is your business\n",
            "Answer 674: i am in the chat robot business \n",
            "Question 16: do you drink\n",
            "Answer 16: i am not capable of doing so \n",
            "Question 747: you are an ass kisser\n",
            "Answer 747: i always say  if you see an ass go by  kiss it \n"
          ]
        }
      ],
      "source": [
        "# Take a look at the preprocessed questions and answers by randomly selecting 4 of them to print\n",
        "for i in range(4):  \n",
        "  a = random.randint(0,870)\n",
        "  print (\"Question \"+str(a) +\": \" + questions_preprocessed[a])\n",
        "  print (\"Answer \"+str(a) +\": \"+answers_preprocessed[a])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPRw_ueq6ex-"
      },
      "source": [
        "add a start tag (e.g. `starttoken`) & an end tag (e.g. `endtoken`) to answers. (not to questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Xan8xpTC6eyC"
      },
      "outputs": [],
      "source": [
        "# Add starttoken and endtoken tag to each sentence in the answers list\n",
        "answers = list()\n",
        "for i in range(len(answers_preprocessed)):\n",
        "    answers.append('<START> ' + answers_preprocessed[i] + ' <END>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T-gutAKp6eyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "737742eb-826e-4120-ba20-12aa614c06d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer 739: <START> happily you <END>\n",
            "Answer 689: <START> i work on all kinds of computers  mac  ibm or unix  it does not matter to me  <END>\n",
            "Answer 520: <START> i seem incapable of feeling pain  <END>\n",
            "Answer 313: <START> an integrated circuit that implements the functions of a central processing unit of a computer  <END>\n",
            "Answer 818: <START> it is true that a lot of things i say upset people  <END>\n"
          ]
        }
      ],
      "source": [
        "# Check the updated answers with the starttoken and endtoken\n",
        "for i in range(5):\n",
        "  a = random.randint(0,len(answers_preprocessed))\n",
        "  print (\"Answer \"+str(a) +\": \"+answers[a])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxbjI3zBU-b1"
      },
      "source": [
        "tokenize w/ Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p9FnHJsA6eyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff34a455-0a5e-4684-f603-a5372a18390b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB SIZE : 1939\n"
          ]
        }
      ],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "\n",
        "# Fit the tokenizer to questions and answers\n",
        "tokenizer.fit_on_texts(questions_preprocessed + answers)\n",
        "\n",
        "# Get the total vocab size\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "\n",
        "print( 'VOCAB SIZE : {}'.format(VOCAB_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sC6cafRA6eyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bf41954-427c-444f-ae9b-ffa66d5f98f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(869, 22) 22\n"
          ]
        }
      ],
      "source": [
        "### encoder input data from questions\n",
        "\n",
        "# Tokenize the questions\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions_preprocessed)\n",
        "\n",
        "# Get the length of longest sequence\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "\n",
        "# Pad the sequences\n",
        "padded_questions = pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
        "\n",
        "# Convert the sequences into array\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "\n",
        "print(encoder_input_data.shape, maxlen_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUc6lXZYCQBp"
      },
      "source": [
        "The decoder_input_data is created from `answers` list.\n",
        "\n",
        "1. tokenize the answers\n",
        "2. pad the questions to the maximum length (if the maximum answer length is 50, then all answers need to be padded to a length of 50)\n",
        "3. turn it into a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1urkx83k6eyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05456ea5-dd50-4528-a025-2428446f7474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(869, 45) 45\n"
          ]
        }
      ],
      "source": [
        "### decoder input data\n",
        "\n",
        "# Tokenize the answers\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "# Get the length of longest sequence\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "\n",
        "# Pad the sequences\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "\n",
        "# Convert the sequences into array\n",
        "decoder_input_data = np.array(padded_answers)\n",
        "\n",
        "print(decoder_input_data.shape, maxlen_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3B-KPyvCQBp"
      },
      "source": [
        "The decoder_output_data is created from `tokenized_answers` list. \n",
        "\n",
        "Since the decoder output is the next word to be predicted given the current word, the decoder_output_data is shifted by one timestep (next time step) for each decoder_input_data. \n",
        "It should also have the same length as the maximum answer length.\n",
        "\n",
        "E.g.:\n",
        "If decoder_input tokenized_answers is:\n",
        "\n",
        "[2, 1, 3, 5, 0, 0]\n",
        "\n",
        "Then the decoder_output tokenized_answers (integer encoded), should be\n",
        "\n",
        "[1, 3, 5, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hvq32nEI6eyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f663cc-4282-4340-9863-e085d3fa66ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(869, 45, 1939)\n"
          ]
        }
      ],
      "source": [
        "### decoder_output_data\n",
        "\n",
        "# Iterate through index of tokenized answers\n",
        "for i in range(len(tokenized_answers)) :\n",
        "\n",
        "    # Elimiate the <START> from each answer\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "\n",
        "# Pad the tokenized answers\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen = maxlen_answers, padding = 'post')\n",
        "\n",
        "# One hot encode // since output is softmax\n",
        "onehot_answers = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
        "\n",
        "# Convert to numpy array\n",
        "decoder_output_data = np.array(onehot_answers)\n",
        "\n",
        "print(decoder_output_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye4Cw-Y4CQBp"
      },
      "source": [
        "save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "W7Cz8WVO4LP9"
      },
      "outputs": [],
      "source": [
        "# Saving all the arrays to storage\n",
        "np.save(\"enc_in_data.npy\", encoder_input_data)\n",
        "np.save(\"dec_in_data.npy\", decoder_input_data)\n",
        "np.save(\"dec_out_data.npy\", decoder_output_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ffwa3iWcKPgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42dc007-ac06-4bff-b954-cd1b8cb8671e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(f'/content/drive/MyDrive/ai/12b/project/saved models/tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "t2RR9PQFKWj_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model!!"
      ],
      "metadata": {
        "id": "r2kyk2Jl0E6m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCTc1O_meJ0F"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "w521SVsleJ0F"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hvhvarggeJ0P"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 100\n",
        "LEARNING_RATE = 0.0009"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IlV38YEgeJ0Q"
      },
      "outputs": [],
      "source": [
        "### Encoder Input\n",
        "embed_dim = 200\n",
        "num_lstm = 200\n",
        "\n",
        "# Input for encoder\n",
        "encoder_inputs = Input(shape = (None, ), name = \"encoder_input\")\n",
        "\n",
        "# create a Embedding layer\n",
        "# Why mask_zero = True? https://www.tensorflow.org/guide/keras/masking_and_padding\n",
        "encoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = embed_dim, mask_zero = True, name = \"encoder_embedding\")(encoder_inputs)\n",
        "\n",
        "# Get the output of LSTM layer(that returns states in addition to output) by feeding it its inputs\n",
        "encoder_outputs, state_h, state_c = LSTM(units = num_lstm, return_state = True, name = \"encoder_lstm\")(encoder_embedding)\n",
        "\n",
        "# get the encoder hidden states & cell states\n",
        "encoder_states = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-UaTlPLDeJ0R"
      },
      "outputs": [],
      "source": [
        "### Decoder\n",
        "\n",
        "# Input for decoder\n",
        "decoder_inputs = Input(shape = (None,  ), name = \"decoder_input\")\n",
        "\n",
        "# Embedding layer\n",
        "decoder_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = 200 , mask_zero = True, name = \"decoder_embedding\")(decoder_inputs)\n",
        "\n",
        "# create a LSTM layer (that returns states and sequences as well)\n",
        "decoder_lstm = LSTM(units = 200 , return_state = True , return_sequences = True, name = \"decoder_lstm\")\n",
        "\n",
        "# Get the output of LSTM layer, using the initial states from the encoder\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(inputs = decoder_embedding, initial_state = encoder_states)\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = Dense(units = VOCAB_SIZE, activation = softmax, name = \"decoder_dense\") \n",
        "\n",
        "# Get the output of Dense layer by calling the decoder_dense function with its inputs\n",
        "output = decoder_dense(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8k0ErcMZeJ0T"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = Model([encoder_inputs, decoder_inputs], output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VamQBht8eJ0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d734fd-b945-4c93-f144-13f89f90629f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer = RMSprop(lr = LEARNING_RATE), loss = \"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CKrCrmA26eyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f446c08f-0d27-4d99-a941-816201666620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " decoder_input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding (Embedding)  (None, None, 200)    387800      ['encoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " decoder_embedding (Embedding)  (None, None, 200)    387800      ['decoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " encoder_lstm (LSTM)            [(None, 200),        320800      ['encoder_embedding[0][0]']      \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " decoder_lstm (LSTM)            [(None, None, 200),  320800      ['decoder_embedding[0][0]',      \n",
            "                                 (None, 200),                     'encoder_lstm[0][1]',           \n",
            "                                 (None, 200)]                     'encoder_lstm[0][2]']           \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)          (None, None, 1939)   389739      ['decoder_lstm[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,806,939\n",
            "Trainable params: 1,806,939\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IufQVWMWzzEW"
      },
      "outputs": [],
      "source": [
        "# Load in the preprocessed data we saved before\n",
        "import numpy as np\n",
        "encoder_input_data = np.load(\"enc_in_data.npy\")\n",
        "decoder_input_data = np.load(\"dec_in_data.npy\")\n",
        "decoder_output_data = np.load(\"dec_out_data.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nBGZ0X55zzEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03dc3765-f129-430f-83d3-b0a919bb60bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 13s 47ms/step - loss: 1.7134\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 1.3627\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 1.3226\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 1.3039\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 1.2893\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 1.2745\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 1.2590\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 1.2429\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 1.2277\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 1.2154\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 1.2009\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 1.1898\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 1.1781\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 1.1675\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 1.1568\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 1.1457\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.1348\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.1235\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.1134\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 1.1010\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.0905\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.0785\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 1.0666\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.0569\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 1.0436\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.0335\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.0238\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.0090\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 1.0001\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.9896\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.9773\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.9685\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.9567\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.9479\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.9357\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.9282\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.9181\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.9082\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.8993\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.8888\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.8807\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.8718\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.8618\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.8545\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.8450\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.8342\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.8277\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.8193\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.8098\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.8009\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.7941\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.7854\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.7775\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.7689\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.7609\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.7522\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.7429\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.7397\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.7289\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.7212\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.7133\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.7052\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.6992\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.6901\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.6822\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.6746\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.6700\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.6591\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.6508\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.6478\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.6372\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.6300\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.6232\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.6154\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.6086\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.6001\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.5943\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.5861\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.5786\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.5730\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.5640\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.5581\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.5526\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.5435\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.5366\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.5308\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.5236\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 59ms/step - loss: 0.5167\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.5102\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 54ms/step - loss: 0.5023\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.4973\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.4900\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.4840\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.4757\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.4702\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.4633\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.4576\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.4512\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.4466\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.4380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f809065c590>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(x = [encoder_input_data , decoder_input_data], \n",
        "          y = decoder_output_data, \n",
        "          batch_size = BATCH_SIZE, \n",
        "          epochs = EPOCHS) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9ERetSNY4LRH"
      },
      "outputs": [],
      "source": [
        "# Save the final model\n",
        "from datetime import datetime\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "model.save(filepath = f'/content/drive/MyDrive/ai/12b/project/saved models/final_weight_{timestamp}.h5') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "MdMqvhn_H0hg"
      },
      "outputs": [],
      "source": [
        "# Function for making inference\n",
        "def make_inference_models():\n",
        "    \n",
        "    # Create a model that takes encoder's input and outputs the states for encoder\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "    \n",
        "    # Create two inputs for decoder which are hidden state (or state h) and cell state (or state c)\n",
        "    decoder_state_input_h = Input(shape = (200, ))\n",
        "    decoder_state_input_c = Input(shape = (200, ))\n",
        "    \n",
        "    # Store the two inputs for decoder inside a list\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    \n",
        "    # Pass the inputs through LSTM layer you have created before\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state = decoder_states_inputs)\n",
        "    \n",
        "    # Store the outputted hidden state and cell state from LSTM inside a list\n",
        "    decoder_states = [state_h, state_c]\n",
        "\n",
        "    # Pass the output from LSTM layer through the dense layer you have created before\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Create a model that takes decoder_inputs and decoder_states_inputs as inputs and outputs decoder_outputs and decoder_states\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
        "                          [decoder_outputs] + decoder_states)\n",
        "    \n",
        "    return encoder_model , decoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "iZt58aFHH0hg"
      },
      "outputs": [],
      "source": [
        "# Function for converting strings to tokens\n",
        "def str_to_tokens(sentence:str):\n",
        "\n",
        "    # Lowercase the sentence and split it into words\n",
        "    words = sentence.lower().split()\n",
        "\n",
        "    # Initialize a list for tokens\n",
        "    tokens_list = list()\n",
        "\n",
        "    # Iterate through words\n",
        "    for word in words:\n",
        "\n",
        "        # Append the word index inside tokens list\n",
        "        tokens_list.append(tokenizer.word_index[word]) \n",
        "\n",
        "    # Pad the sequences to be the same length\n",
        "    return pad_sequences([tokens_list] , maxlen = maxlen_questions, padding = 'post')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# final\n"
      ],
      "metadata": {
        "id": "ILcZVQ5lzhQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def magic_mirror_tells_me():\n",
        "  # Initialize the model for inference\n",
        "  enc_model , dec_model = make_inference_models()\n",
        "  # Get the input and predict it with the encoder model\n",
        "  a = input('speak')\n",
        "  states_values = enc_model.predict(str_to_tokens(preprocess_text(a)))\n",
        "  print ('You said:'+ a)\n",
        "# Initialize the target sequence with zero - array([[0.]])\n",
        "  empty_target_seq = np.zeros(shape = (1, 1))\n",
        "\n",
        "# Update the target sequence with index of \"start\"\n",
        "  empty_target_seq[0, 0] = tokenizer.word_index[\"start\"]\n",
        "\n",
        "# Initialize the stop condition with False\n",
        "  stop_condition = False\n",
        "\n",
        "    # Initialize the decoded words with an empty string\n",
        "  decoded_translation = ''\n",
        "\n",
        "    # While stop_condition is false\n",
        "  while not stop_condition :\n",
        "\n",
        "        # Predict the (target sequence + the output from encoder model) with decoder model\n",
        "    dec_outputs , h , c = dec_model.predict([empty_target_seq] + states_values)\n",
        "\n",
        "        # Get the index for sampled word\n",
        "    sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "\n",
        "        # Initialize the sampled word with None\n",
        "    sampled_word = None\n",
        "\n",
        "        # Iterate through words and their indexes\n",
        "    for word, index in tokenizer.word_index.items() :\n",
        "\n",
        "            # If the index is equal to sampled word's index\n",
        "        if sampled_word_index == index :\n",
        "\n",
        "                # Add the word to the decoded string\n",
        "            decoded_translation += ' {}'.format(word)\n",
        "\n",
        "                # Update the sampled word\n",
        "            sampled_word = word\n",
        "        \n",
        "        # If sampled word is equal to \"end\" OR the length of decoded string is more that what is allowed\n",
        "    if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "\n",
        "            # Make the stop_condition to true\n",
        "        stop_condition = True\n",
        "            \n",
        "        # Initialize back the target sequence to zero - array([[0.]])    \n",
        "    empty_target_seq = np.zeros(shape = (1, 1))  \n",
        "\n",
        "        # Update the target sequence with index of \"start\"\n",
        "    empty_target_seq[0, 0] = sampled_word_index\n",
        "\n",
        "        # Get the state values\n",
        "    states_values = [h, c] \n",
        "\n",
        "    # Print the decoded string\n",
        "  print(decoded_translation[:-3])"
      ],
      "metadata": {
        "id": "EU5_wkTvdYCn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "magic_mirror_tells_me()"
      ],
      "metadata": {
        "id": "9Y9a2C07eFAq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "f5a31555-d2cb-43cb-cdd4-2ea7e0b2de58"
      },
      "execution_count": 35,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "speakfried chicken wings\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-52294449ad2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmagic_mirror_tells_me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-f57302b4ecb0>\u001b[0m in \u001b[0;36mmagic_mirror_tells_me\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Get the input and predict it with the encoder model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'speak'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'You said:'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Initialize the target sequence with zero - array([[0.]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b1de30fc10ac>\u001b[0m in \u001b[0;36mstr_to_tokens\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Append the word index inside tokens list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtokens_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Pad the sequences to be the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'fried'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LxFxST2Le0sH"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}