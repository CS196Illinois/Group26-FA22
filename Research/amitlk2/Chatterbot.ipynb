{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import chatterbot\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from LeXmo import LeXmo\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_text(text):\n",
    "    text = text.replace(\"â\\x80\\x99\", \"'\")\n",
    "    if \"ð\" in text:\n",
    "        for word in text.split():\n",
    "            if \"ð\" in word:\n",
    "                text = text.replace(word, \"EMOJI\")\n",
    "    return text\n",
    "\n",
    "\n",
    "#This chunk of code reads the json files for FACEBOOK MESSENGER MESSAGES\n",
    "person = \"Amit\" #Which person do you want the chat bot to replicate?\n",
    "sender = \"Amit Krishnaiyer\"\n",
    "path = \"../Messages/\" + person\n",
    "conversation_list = [] #List of all conversations in an organized format\n",
    "speaker_list = [] #List of everything the speaker said\n",
    "for filename in os.listdir(path):\n",
    "    if filename[-4:] == \"json\":\n",
    "        conversation = []\n",
    "        f = open(path + \"/\" + filename)\n",
    "        data = json.load(f)[\"messages\"]\n",
    "        for message in data:\n",
    "            if \"content\" in message:\n",
    "                the_message = fix_text(message[\"content\"])\n",
    "                conversation.append((the_message, message[\"sender_name\"]))\n",
    "                if sender == message[\"sender_name\"]:\n",
    "                    speaker_list.append(the_message)\n",
    "        conversation_list.append(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('u', 111), ('t', 82), ('yea', 80), ('ur', 74), ('cuz', 66), ('like', 65), ('lol', 53), ('get', 52), (\"i'm\", 49), ('n', 47), ('also', 42), ('lke', 42), ('thnk', 38), ('rlly', 38), ('w', 37), ('ab', 29), ('emoji', 28), ('go', 28), ('well', 27), ('tho', 27), (\"'m\", 26), ('good', 26), ('f', 26), ('would', 25), ('one', 25), ('got', 24), ('see', 24), ('even', 24), ('wanna', 23), (\"that's\", 23)]\n"
     ]
    }
   ],
   "source": [
    "#This block of code gets the speaker's most common used words\n",
    "word_lis = []\n",
    "for message in speaker_list:\n",
    "    message = preprocesser(message, True, False, False, True, True)# (text, lower, punctuation, named_entity, white_space, stop_words)\n",
    "    if message.strip() != \"amit  played,\":\n",
    "        words = message.split()\n",
    "        for word in words:\n",
    "            if word != \"\":\n",
    "                word_lis.append(word)\n",
    "\n",
    "# Pass the split_it list to instance of Counter class.\n",
    "\n",
    "Counter = Counter(word_lis)\n",
    "  \n",
    "# most_common() produces k frequently encountered\n",
    "# input values and their respective counts.\n",
    "most_occur = Counter.most_common(30)\n",
    "  \n",
    "print(most_occur)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As President and ORGANIZATION of ORGANIZATION, GPE brings over 15 years of global business experience within client services having worked and lived in GPE, GPE, GPE, and GPE. GPE is a mission-driven leader with experience in building organizations and leadership teams with more focus, more growth and more enjoyment. He brings extensive experience working effectively in high-growth environments to set a clear vision and strategy and build strong systems, processes, and infrastructure necessary to sustainably scale businesses to their maximum potential. He brings a highly client-focused mindset and makes it a core part of the company’s goals and values.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Only have to run the code below once\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "'''\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words_set = stopwords.words('english')\n",
    "custom_stop_words = [\"um\", \"uhm\", \"uh\", \"oh\", \"ok\", \"turn!\"]\n",
    "stop_words_set = set(stop_words_set + custom_stop_words)\n",
    "\n",
    "\n",
    "\n",
    "def find_and_replace(sentence): #Named Entity Recognition, replace words with general categories\n",
    "    dic = {}\n",
    "    for sent in nltk.sent_tokenize(sentence):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "             if hasattr(chunk, 'label'):\n",
    "                #print(f\"{' '.join(c[0] for c in chunk):<35} {chunk.label()}\")\n",
    "                thing = ' '.join(c[0] for c in chunk)\n",
    "                dic[thing] = chunk.label()\n",
    "                #print(thing, chunk.label())\n",
    "    for key in dic:\n",
    "        sentence = sentence.replace(key, dic[key])\n",
    "    return sentence\n",
    "\n",
    "def remove_punctuation(word): #Removes the puntuation at the end of the word\n",
    "    if word[0].isalpha() == False and word[0].isdigit() == False:\n",
    "        word = word[1:]\n",
    "    if len(word) > 0:\n",
    "        if word[-1].isalpha() == False and word[-1].isdigit() == False:\n",
    "            word = word[:-1]\n",
    "    return word\n",
    "\n",
    "def replace_text_slang(text):\n",
    "    URL = \"https://www.slicktext.com/blog/2019/02/text-abbreviations-guide/\"\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(\"ol\")\n",
    "    acronyms = results.find_all(\"li\")\n",
    "    dic = {}\n",
    "    for i in range(len(acronyms)):\n",
    "        lis = acronyms[i].text.split(\":\")\n",
    "        lis[0] = lis[0].lower().strip()\n",
    "        lis[1] = lis[1].lower().strip()\n",
    "        dic[lis[0]] = lis[1]\n",
    "    for key in dic:\n",
    "        text.replace(key, dic[key])\n",
    "    return text\n",
    "\n",
    "def preprocesser(text, lower, punctuation, named_entity, white_space, stop_words):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    if punctuation:\n",
    "        text = remove_punctuation(text)\n",
    "    if named_entity:\n",
    "        text = find_and_replace(text)\n",
    "    if white_space:\n",
    "        text = text.strip()\n",
    "    if stop_words:\n",
    "        for word in text.split():\n",
    "            if word in stop_words_set:\n",
    "                text = text.replace(word, \"\")    \n",
    "    return text\n",
    "\n",
    "  \n",
    "sentence = \"As President and CEO of TribalVision, Rahul brings over 15 years of global business experience within client services having worked and lived in New York, Switzerland, Beijing, and California. Rahul is a mission-driven leader with experience in building organizations and leadership teams with more focus, more growth and more enjoyment. He brings extensive experience working effectively in high-growth environments to set a clear vision and strategy and build strong systems, processes, and infrastructure necessary to sustainably scale businesses to their maximum potential. He brings a highly client-focused mindset and makes it a core part of the company’s goals and values.\"\n",
    "\n",
    "print(find_and_replace(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/amit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/amit/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from chatterbot import ChatBot\n",
    "chatbot = ChatBot(person)\n",
    "\n",
    "preprocessors=['chatterbot.preprocessors.clean_whitespace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Trainer: [####################] 100%\n",
      "List Trainer: [####################] 100%\n",
      "Hello!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nWhat do I consider when responding to texts?\\nHigh Priority:\\nEnergy of the message: I try to match energy: Could measure energy of message?\\nWhat is the sentiment of the message?\\nWhat are my interests at the moment?\\nWho are the people I'm talking about the most at the time. \\n\\n\\nBonus: What are they asking for- is it something I can google?\\n\\n\\nLow Priority:\\nWhat time of day is it? Do I text differently at night than day? \\n\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chatterbot.trainers import ListTrainer\n",
    "\n",
    "trainer = ListTrainer(chatbot)\n",
    "\n",
    "trainer.train([\n",
    "    \"Hi there!\",\n",
    "    \"Hello!!\",\n",
    "])\n",
    "\n",
    "trainer.train([\n",
    "    \"Greetings!\",\n",
    "    \"Hello!\",\n",
    "])\n",
    "response = chatbot.get_response(\"Greetings!\")\n",
    "print(response)\n",
    "\n",
    "\n",
    "'''\n",
    "What do I consider when responding to texts?\n",
    "High Priority:\n",
    "Energy of the message: I try to match energy: Could measure energy of message?\n",
    "What is the sentiment of the message?\n",
    "What are my interests at the moment?\n",
    "Who are the people I'm talking about the most at the time. \n",
    "\n",
    "\n",
    "Bonus: What are they asking for- is it something I can google?\n",
    "\n",
    "\n",
    "Low Priority:\n",
    "What time of day is it? Do I text differently at night than day? \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'U r by being good at reading', 'anger': 0.0, 'anticipation': 0.14285714285714285, 'disgust': 0.0, 'fear': 0.0, 'joy': 0.14285714285714285, 'negative': 0.0, 'positive': 0.14285714285714285, 'sadness': 0.0, 'surprise': 0.14285714285714285, 'trust': 0.14285714285714285}\n"
     ]
    }
   ],
   "source": [
    "#Emotional Classification: https://devblogs.microsoft.com/cse/2015/11/29/emotion-detection-and-recognition-from-text-using-deep-learning/\n",
    "#Five categories: Happy, Angry, Sad, Surprise, Fear\n",
    "t = random.choice(speaker_list)\n",
    "emo=LeXmo.LeXmo(t)\n",
    "print(emo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get(\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\")\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "inputs = []\n",
    "outputs = []\n",
    "#print(soup.text)\n",
    "for line in soup.text.split(\"\\n\"):\n",
    "    count = 0\n",
    "    temp_in = []\n",
    "    for num in line.split(\",\"):\n",
    "        count += 1\n",
    "        if count < 9:\n",
    "            temp_in.append(float(num))\n",
    "        else:\n",
    "            outputs.append(float(num))\n",
    "    inputs.append(temp_in)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(inputs, outputs, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
